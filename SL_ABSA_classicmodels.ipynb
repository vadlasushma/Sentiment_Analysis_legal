{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SL-ABSA-classicmodels.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO5c0wV7JfcVFRTTvVn7sX6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vadlasushma/Sentiment_Analysis_legal/blob/main/SL_ABSA_classicmodels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49pXzmDXrzst",
        "outputId": "31ad613d-0e29-4592-a93d-eb53a66e310e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "# pre-trained vector for w2v\n",
        "import gensim\n",
        "import gensim.downloader as api\n",
        "nlp = api.load('word2vec-google-news-300')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnSXCtXLcEUf"
      },
      "source": [
        "#clean the dataset\n",
        "def clean_dataset(dataSet):\n",
        "    assert isinstance(dataSet, pd.DataFrame)\n",
        "    dataSet.dropna(inplace=True)\n",
        "    indices_to_keep = ~dataSet.isin([np.nan, np.inf, -np.inf]).any(1)\n",
        "    return dataSet[indices_to_keep]\n",
        "# preprocess the dataset\n",
        "def pre_process(dataset) :\n",
        "  dataset['Sentence']=dataset['Sentence'].str.lower()\n",
        "  dataset['Aspect']=dataset['Aspect'].str.lower()\n",
        "  vocab = dataset['Aspect'].str.lstrip().unique()\n",
        "  return dataset,vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3K7imR9RqmV"
      },
      "source": [
        "#W2V\n",
        "#BOW based approaches\n",
        "nlp.init_sims(replace=True) # calling for using syn0norm\n",
        "\n",
        "def word_averaging(wv, words):\n",
        "    all_words, mean = set(), []\n",
        "    \n",
        "    for word in words:\n",
        "        if isinstance(word, np.ndarray):\n",
        "            mean.append(word)\n",
        "        elif word in wv.vocab:\n",
        "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
        "            all_words.add(wv.vocab[word].index)\n",
        "\n",
        "    if not mean:\n",
        "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
        "        # FIXME: remove these examples in pre-processing\n",
        "        return np.zeros(wv.vector_size,)\n",
        "\n",
        "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
        "    return mean\n",
        "\n",
        "def  word_averaging_list(wv, text_list):\n",
        "    return np.vstack([word_averaging(wv, post) for post in text_list ])\n",
        "\n",
        "# Tokenize, and apply word vector averaging to tokenized text\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "def w2v_tokenize_text(text):\n",
        "    tokens = []\n",
        "    for sent in nltk.sent_tokenize(text, language='english'):\n",
        "        for word in nltk.word_tokenize(sent, language='english'):\n",
        "            if len(word) < 2:\n",
        "                continue\n",
        "            tokens.append(word)\n",
        "    return tokens\n",
        "    \n",
        "\n",
        "X_train_tokenized = train_data['Sentence'].apply(lambda x: w2v_tokenize_text(x)).values\n",
        "X_test_tokenized = test_data['Sentence'].apply(lambda x: w2v_tokenize_text(x)).values\n",
        "\n",
        "X_train_word_average = word_averaging_list(nlp,X_train_tokenized)\n",
        "X_test_word_average = word_averaging_list(nlp,X_test_tokenized)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4921zwHmh1W"
      },
      "source": [
        "#def for finding the position of the aspect word in the sentence\n",
        "import regex as re\n",
        "def search_pos(sentences,party) :\n",
        "  temp = sentences\n",
        "  # print(temp)\n",
        "  # for i in range(len(aspect)) :\n",
        "  regex1 = re.compile(r'\\b' + party + '\\'s' + r'\\b', re.IGNORECASE)\n",
        "  matched1 = regex1.search(temp)\n",
        "  regex2 = re.compile(r'\\b' + party + r'\\b', re.IGNORECASE)\n",
        "  matched2 = regex2.search(temp)\n",
        "  if matched1 is not None and bool(matched1.group()):\n",
        "    return matched1.start()\n",
        "  else :\n",
        "    if matched2 is not None and bool(matched2.group()):\n",
        "      return matched2.start()\n",
        "    \n",
        "  return 0   \n",
        "# def for finding if aspect is present in sentence ()\n",
        "def search(sentences,party) :\n",
        "  temp = sentences\n",
        "  # print(temp)\n",
        "  # for i in range(len(aspect)) :\n",
        "  regex1 = re.compile(r'\\b' + party + '\\'s' + r'\\b', re.IGNORECASE)\n",
        "  matched1 = regex1.search(temp)\n",
        "  regex2 = re.compile(r'\\b' + party + r'\\b', re.IGNORECASE)\n",
        "  matched2 = regex2.search(temp)\n",
        "  if matched1 is not None and bool(matched1.group()):\n",
        "    return 1\n",
        "  else :\n",
        "    if matched2 is not None and bool(matched2.group()):\n",
        "      return 1\n",
        "    \n",
        "  return 0   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Diiq1Ts4bkvw"
      },
      "source": [
        "#Reading Dataset\n",
        "# import csv\n",
        "# train_data = pd.read_csv(\"/content/drive/My Drive/Legal/Masked-sent-part-strat-nodupes-train.csv\")\n",
        "# test_data =  pd.read_csv(\"/content/drive/My Drive/Legal/Masked-sent-part-strat-nodupes-test.csv\")\n",
        "import csv\n",
        "train_data = pd.read_csv(\"/content/drive/My Drive/Datasets/SABSA-new-augmented-train-after-sen_part-stra.csv\")\n",
        "test_data =  pd.read_csv(\"/content/drive/My Drive/Datasets/SABSA-new-test-after-sen_part-Stra.csv\")\n",
        "# ,encoding='cp1252'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxi-x6mwV_Mj"
      },
      "source": [
        "#Cleaning dataset\n",
        "train_data =clean_dataset(train_data )\n",
        "print(train_data)\n",
        "test_data =clean_dataset(test_data )\n",
        "print(test_data)\n",
        "#preprocess dataset\n",
        "train_data,train_vocab=pre_process(train_data)\n",
        "print(train_data)\n",
        "test_data,test_vocab=pre_process(test_data)\n",
        "print(test_data)\n",
        "#making vocab by combining vocabs of train and test data\n",
        "vocab_aspect = np.hstack((train_vocab,test_vocab))\n",
        "print(len(vocab_aspect))\n",
        "vocab_aspect=np.unique(vocab_aspect)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PElW6cbmhle"
      },
      "source": [
        "# Making of aspect matrix (postion of aspect and if aspect is present in sentence or not)\n",
        "# 1. position of aspect in sent\n",
        "l1 = []\n",
        "for sent in train_data['Sentence']:\n",
        "  temp = []\n",
        "  for word in vocab_aspect :\n",
        "    temp.append(search_pos(sent,word))\n",
        "  l1.append(temp)\n",
        "l2 = []\n",
        "for sent in test_data['Sentence']:\n",
        "  temp = []\n",
        "  for word in vocab_aspect :\n",
        "    temp.append(search_pos(sent,word))\n",
        "  l2.append(temp)\n",
        "  # 2. if aspect is present or not in sentence\n",
        "l3 = []\n",
        "for sent in train_data['Sentence']:\n",
        "  temp = []\n",
        "  for word in vocab_aspect :\n",
        "    temp.append(search(sent,word))\n",
        "  l3.append(temp)\n",
        "\n",
        "l4 = []\n",
        "for sent in test_data['Sentence']:\n",
        "  temp = []\n",
        "  for word in vocab_aspect :\n",
        "    temp.append(search(sent,word))\n",
        "  l4.append(temp)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYodWXjUMI33"
      },
      "source": [
        "#lists to dataframes (aspect matrix)\n",
        "aspect_matrix_train_pos = pd.DataFrame(l1, columns=vocab_aspect)\n",
        "aspect_matrix_test_pos = pd.DataFrame(l2, columns=vocab_aspect)\n",
        "aspect_matrix_train = pd.DataFrame(l3, columns=vocab_aspect)\n",
        "aspect_matrix_test = pd.DataFrame(l4, columns=vocab_aspect)\n",
        "# lists to dataframes (w2v matrix)\n",
        "vect2 = pd.DataFrame(X_train_word_average)\n",
        "vect1 = pd.DataFrame(X_test_word_average)\n",
        "#concatenate w2v matrix, aspect matrix (position + aspect search), party label column from dataset\n",
        "X_train = pd.concat([vect2,aspect_matrix_train,aspect_matrix_train_pos,train_data['PartyLabel']], axis=1)\n",
        "X_test= pd.concat([vect1,aspect_matrix_test,aspect_matrix_test_pos,test_data['PartyLabel']], axis=1)\n",
        "y_train =train_data['Sentiment']\n",
        "y_test =test_data['Sentiment']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWosob3IkvF6"
      },
      "source": [
        "## Model Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WovHMZAhX9KU"
      },
      "source": [
        "#logistic regression\n",
        "\n",
        "logclf = LogisticRegression(solver='liblinear')\n",
        "logclf.fit(X_train, y_train)\n",
        "y_pred = logclf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "all_accuracy = []\n",
        "from sklearn.metrics import accuracy_score\n",
        "score =accuracy_score(y_test,y_pred)\n",
        "all_accuracy.append(score)\n",
        "# import the metrics class\n",
        "from sklearn import metrics\n",
        "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
        "cnf_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0h2BU50qloa"
      },
      "source": [
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# for roc_plot(model)\n",
        "fp0 = []\n",
        "fp1 = []\n",
        "fp2 = []\n",
        "tp0 = []\n",
        "tp1 = []\n",
        "tp2 = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSsd7rtZqQIZ"
      },
      "source": [
        "#Roc curve\n",
        "pred = logclf.predict(X_test)\n",
        "pred_prob = logclf.predict_proba(X_test)\n",
        "\n",
        "# roc curve for classes\n",
        "fpr = {}\n",
        "tpr = {}\n",
        "thresh ={}\n",
        "\n",
        "n_class = 3\n",
        "\n",
        "for i in range(n_class):    \n",
        "    fpr[i], tpr[i], thresh[i] = roc_curve(y_test, pred_prob[:,i], pos_label=i)\n",
        "\n",
        "fp0.append(fpr[0])\n",
        "fp1.append(fpr[1])\n",
        "tp0.append(tpr[0])\n",
        "tp1.append(tpr[1])\n",
        "fp2.append(fpr[2])\n",
        "tp2.append(tpr[2])\n",
        "# plotting    \n",
        "plt.plot(fpr[0], tpr[0], linestyle='--',color='orange', label='Class 0 vs Rest')\n",
        "plt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='Class 1 vs Rest')\n",
        "plt.plot(fpr[2], tpr[2], linestyle='--',color='blue', label='Class 2 vs Rest')\n",
        "plt.title('Multiclass ROC curve')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive rate')\n",
        "plt.legend(loc='best')\n",
        "plt.savefig('Multiclass ROC',dpi=300);    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ylZXDchsCI-"
      },
      "source": [
        "# Decision Tree Classifier\n",
        "from sklearn.tree import DecisionTreeClassifier \n",
        "dtclf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3)\n",
        "\n",
        "# Train Decision Tree Classifer\n",
        "dtclf.fit(X_train,y_train)\n",
        "\n",
        "#Predict the response for test dataset\n",
        "y_pred = dtclf.predict(X_test)\n",
        "\n",
        "# Model Accuracy, how often is the classifier correct?\n",
        "print(classification_report(y_test, y_pred))\n",
        "# all_accuracy = []\n",
        "from sklearn.metrics import accuracy_score\n",
        "score =accuracy_score(y_test,y_pred)\n",
        "all_accuracy.append(score)\n",
        "# import the metrics class\n",
        "from sklearn import metrics\n",
        "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
        "cnf_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3nDjgUzw18m"
      },
      "source": [
        "#roc curve\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "pred = dtclf.predict(X_test)\n",
        "pred_prob = dtclf.predict_proba(X_test)\n",
        "# from collections import Counter\n",
        "# print(Counter(pred))\n",
        "# # roc curve for classes\n",
        "fpr = {}\n",
        "tpr = {}\n",
        "thresh ={}\n",
        "\n",
        "n_class = 3\n",
        "\n",
        "for i in range(n_class):    \n",
        "    fpr[i], tpr[i], thresh[i] = roc_curve(y_test, pred_prob[:,i], pos_label=i)\n",
        "print(fpr[2])    \n",
        "fp0.append(fpr[0])\n",
        "fp1.append(fpr[1])\n",
        "# fp2.append(fpr[2])\n",
        "tp0.append(tpr[0])\n",
        "tp1.append(tpr[1])\n",
        "# tp2.append(tpr[2])\n",
        "fp2.append(fpr[2])\n",
        "tp2.append(tpr[2])\n",
        "# plotting    \n",
        "plt.plot(fpr[0], tpr[0], linestyle='--',color='orange', label='Class 0 vs Rest')\n",
        "plt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='Class 1 vs Rest')\n",
        "plt.plot(fpr[2], tpr[2], linestyle='--',color='blue', label='Class 2 vs Rest')\n",
        "plt.title('Multiclass ROC curve')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive rate')\n",
        "plt.legend(loc='best')\n",
        "plt.savefig('Multiclass ROC',dpi=300);    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ts0wbrI-sqr-"
      },
      "source": [
        "#RandomForestClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rfclf=RandomForestClassifier(n_estimators=100)\n",
        "\n",
        "rfclf.fit(X_train,y_train)\n",
        "\n",
        "y_pred = rfclf.predict(X_test)\n",
        "\n",
        "# Model Accuracy, how often is the classifier correct?\n",
        "print(classification_report(y_test, y_pred))\n",
        "# all_accuracy = []\n",
        "from sklearn.metrics import accuracy_score\n",
        "score =accuracy_score(y_test,y_pred)\n",
        "all_accuracy.append(score)\n",
        "# import the metrics class\n",
        "from sklearn import metrics\n",
        "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
        "cnf_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEnkmfj4MTWr"
      },
      "source": [
        "#roc curve\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "pred = rfclf.predict(X_test)\n",
        "pred_prob = rfclf.predict_proba(X_test)\n",
        "\n",
        "# roc curve for classes\n",
        "fpr = {}\n",
        "tpr = {}\n",
        "thresh ={}\n",
        "\n",
        "n_class = 3\n",
        "\n",
        "for i in range(n_class):    \n",
        "    fpr[i], tpr[i], thresh[i] = roc_curve(y_test, pred_prob[:,i], pos_label=i)\n",
        "\n",
        "fp0.append(fpr[0])\n",
        "fp1.append(fpr[1])\n",
        "tp0.append(tpr[0])\n",
        "tp1.append(tpr[1]) \n",
        "fp2.append(fpr[2])\n",
        "tp2.append(tpr[2])   \n",
        "# plotting    \n",
        "plt.plot(fpr[0], tpr[0], linestyle='--',color='orange', label='Class 0 vs Rest')\n",
        "plt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='Class 1 vs Rest')\n",
        "plt.plot(fpr[2], tpr[2], linestyle='--',color='blue', label='Class 2 vs Rest')\n",
        "plt.title('Multiclass ROC curve')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive rate')\n",
        "plt.legend(loc='best')\n",
        "plt.savefig('Multiclass ROC',dpi=300);    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tokRuQuTsrga"
      },
      "source": [
        "#SVM\n",
        "from sklearn import svm\n",
        "\n",
        "#Create a svm Classifier\n",
        "svmclf = svm.SVC(kernel='linear',probability= True) # Linear Kernel\n",
        "\n",
        "#Train the model using the training sets\n",
        "svmclf.fit(X_train, y_train)\n",
        "\n",
        "#Predict the response for test dataset\n",
        "y_pred = svmclf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "# all_accuracy = []\n",
        "from sklearn.metrics import accuracy_score\n",
        "score =accuracy_score(y_test,y_pred)\n",
        "all_accuracy.append(score)\n",
        "# import the metrics class\n",
        "from sklearn import metrics\n",
        "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
        "cnf_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52D_AIeGwxZP"
      },
      "source": [
        "#roc curve\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "pred = svmclf.predict(X_test)\n",
        "pred_prob = svmclf.predict_proba(X_test)\n",
        "\n",
        "# roc curve for classes\n",
        "fpr = {}\n",
        "tpr = {}\n",
        "thresh ={}\n",
        "\n",
        "n_class = 3\n",
        "\n",
        "for i in range(n_class):    \n",
        "    fpr[i], tpr[i], thresh[i] = roc_curve(y_test, pred_prob[:,i], pos_label=i)\n",
        "    \n",
        "fp0.append(fpr[0])\n",
        "fp1.append(fpr[1])\n",
        "tp0.append(tpr[0])\n",
        "tp1.append(tpr[1])\n",
        "fp2.append(fpr[2])\n",
        "tp2.append(tpr[2])\n",
        "# plotting    \n",
        "plt.plot(fpr[0], tpr[0], linestyle='--',color='orange', label='Class 0 vs Rest')\n",
        "plt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='Class 1 vs Rest')\n",
        "plt.plot(fpr[2], tpr[2], linestyle='--',color='blue', label='Class 2 vs Rest')\n",
        "plt.title('Multiclass ROC curve')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive rate')\n",
        "plt.legend(loc='best')\n",
        "plt.savefig('Multiclass ROC',dpi=300);   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUQrD5PwDZfd"
      },
      "source": [
        "#KNeighborsClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn = KNeighborsClassifier(n_neighbors=7)\n",
        " \n",
        "knn.fit(X_train, y_train)\n",
        "y_pred = knn.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# all_accuracy = []\n",
        "from sklearn.metrics import accuracy_score\n",
        "score =accuracy_score(y_test,y_pred)\n",
        "all_accuracy.append(score)\n",
        "# import the metrics class\n",
        "from sklearn import metrics\n",
        "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
        "cnf_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqTMvXNQ5Y_f"
      },
      "source": [
        "#roc curve\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "pred = knn.predict(X_test)\n",
        "pred_prob = knn.predict_proba(X_test)\n",
        "\n",
        "# roc curve for classes\n",
        "fpr = {}\n",
        "tpr = {}\n",
        "thresh ={}\n",
        "\n",
        "n_class = 3\n",
        "\n",
        "for i in range(n_class):    \n",
        "    fpr[i], tpr[i], thresh[i] = roc_curve(y_test, pred_prob[:,i], pos_label=i)\n",
        "    \n",
        "fp0.append(fpr[0])\n",
        "fp1.append(fpr[1])\n",
        "tp0.append(tpr[0])\n",
        "tp1.append(tpr[1])\n",
        "fp2.append(fpr[2])\n",
        "tp2.append(tpr[2])\n",
        "# plotting    \n",
        "plt.plot(fpr[0], tpr[0], linestyle='--',color='orange', label='Class 0 vs Rest')\n",
        "plt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='Class 1 vs Rest')\n",
        "plt.plot(fpr[2], tpr[2], linestyle='--',color='blue', label='Class 2 vs Rest')\n",
        "plt.title('Multiclass ROC curve')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive rate')\n",
        "plt.legend(loc='best')\n",
        "plt.savefig('Multiclass ROC',dpi=300);   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPyGUvvcfcAS"
      },
      "source": [
        "#Gaussian Classifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "#Create a Gaussian Classifier\n",
        "gnb = GaussianNB()\n",
        "\n",
        "#Train the model using the training sets\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "#Predict the response for test dataset\n",
        "y_pred = gnb.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "# all_accuracy = []\n",
        "from sklearn.metrics import accuracy_score\n",
        "score =accuracy_score(y_test,y_pred)\n",
        "all_accuracy.append(score)\n",
        "# import the metrics class\n",
        "from sklearn import metrics\n",
        "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
        "cnf_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkJGhqmNjcl3"
      },
      "source": [
        "#roc curve\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "pred = gnb.predict(X_test)\n",
        "pred_prob = gnb.predict_proba(X_test)\n",
        "\n",
        "# roc curve for classes\n",
        "fpr = {}\n",
        "tpr = {}\n",
        "thresh ={}\n",
        "\n",
        "n_class = 3\n",
        "\n",
        "for i in range(n_class):    \n",
        "    fpr[i], tpr[i], thresh[i] = roc_curve(y_test, pred_prob[:,i], pos_label=i)\n",
        "    \n",
        "fp0.append(fpr[0])\n",
        "fp1.append(fpr[1])\n",
        "tp0.append(tpr[0])\n",
        "tp1.append(tpr[1])\n",
        "fp2.append(fpr[2])\n",
        "tp2.append(tpr[2])\n",
        "# plotting    \n",
        "plt.plot(fpr[0], tpr[0], linestyle='--',color='orange', label='Class 0 vs Rest')\n",
        "plt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='Class 1 vs Rest')\n",
        "plt.plot(fpr[2], tpr[2], linestyle='--',color='blue', label='Class 2 vs Rest')\n",
        "plt.title('Multiclass ROC curve')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive rate')\n",
        "plt.legend(loc='best')\n",
        "plt.savefig('Multiclass ROC',dpi=300);   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JTf_lsyjfS4"
      },
      "source": [
        "#Perceptron\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# Create a perceptron object with the parameters: 40 iterations (epochs) over the data, and a learning rate of 0.1\n",
        "ppn = Perceptron(n_iter_no_change=40,eta0=0.1,random_state=0)\n",
        "\n",
        "# Train the perceptron\n",
        "ppn.fit(X_train, y_train)\n",
        "# Apply the trained perceptron on the X data to make predicts for the y test data\n",
        "y_pred = ppn.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "# all_accuracy = []\n",
        "from sklearn.metrics import accuracy_score\n",
        "score =accuracy_score(y_test,y_pred)\n",
        "all_accuracy.append(score)\n",
        "# import the metrics class\n",
        "from sklearn import metrics\n",
        "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
        "cnf_matrix\n",
        "print(all_accuracy)\n",
        "# print(fp2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizations"
      ],
      "metadata": {
        "id": "e-gmbL8ULGsw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "so9W3RQVx-S6"
      },
      "source": [
        "classifiers = ['Logisticclf','DecisionTree','RandomForest','SVM','KNN','Naive_bayes','Perceptron']\n",
        "\n",
        "#BOX PLOT for accuracy of all models\n",
        "import matplotlib.pyplot as plt\n",
        "fig = plt.figure(figsize =(10, 5))\n",
        "ax = fig.add_axes([0,0,1,1])\n",
        "# c = ['Logisticclf','DecisionTree','RandomForest','SVM','KNN','Naive_bayes']\n",
        "# a = [0.5966257668711656, 0.48466257668711654, 0.7239263803680982, 0.6794478527607362, 0.5230061349693251, 0.6595092024539877]\n",
        "ax.bar(classifiers,all_accuracy)\n",
        "plt.show()\n",
        "fig = plt.figure(figsize =(7, 5))\n",
        " # Creating axes instance\n",
        "ax = fig.add_axes([0, 0, 1, 1])\n",
        " # Creating plot\n",
        "bp = ax.boxplot(all_accuracy)\n",
        "plt.title('Box Plot of Accuracy of all models', fontweight='bold', fontsize=15) \n",
        "# show plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7DIcmgfzd4z"
      },
      "source": [
        "#Roc curve of all models for class 0\n",
        "fig = plt.figure(figsize=(8,6))\n",
        "\n",
        "for i in range(len(classifiers)-1):\n",
        "    plt.plot(fp0[i], \n",
        "             tp0[i],\n",
        "             label=\"{}\".format(classifiers[i]))\n",
        "    \n",
        "plt.plot([0,1], [0,1], color='orange', linestyle='--')\n",
        "\n",
        "plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.xlabel(\"Flase Positive Rate\", fontsize=15)\n",
        "\n",
        "plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
        "\n",
        "plt.title('ROC Curve Analysis for class 0', fontweight='bold', fontsize=15)\n",
        "plt.legend(prop={'size':10}, loc='lower right')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEH97KC8zkX4"
      },
      "source": [
        "#Roc Curve for all models for class1\n",
        "fig = plt.figure(figsize=(8,6))\n",
        "\n",
        "for i in range(len(classifiers)-1):\n",
        "    plt.plot(fp1[i], \n",
        "             tp1[i],\n",
        "             label=\"{}\".format(classifiers[i]))\n",
        "    \n",
        "plt.plot([0,1], [0,1], color='orange', linestyle='--')\n",
        "\n",
        "plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.xlabel(\"Flase Positive Rate\", fontsize=15)\n",
        "\n",
        "plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
        "\n",
        "plt.title('ROC Curve Analysis for class 1', fontweight='bold', fontsize=15)\n",
        "plt.legend(prop={'size':10}, loc='lower right')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMCsXW_3z1Eg"
      },
      "source": [
        "#Roc curve for all models for class2\n",
        "fig = plt.figure(figsize=(8,6))\n",
        "\n",
        "for i in range(len(classifiers)-1):\n",
        "    plt.plot(fp2[i], \n",
        "             tp2[i],\n",
        "             label=\"{}\".format(classifiers[i]))\n",
        "    \n",
        "plt.plot([0,1], [0,1], color='orange', linestyle='--')\n",
        "\n",
        "plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.xlabel(\"Flase Positive Rate\", fontsize=15)\n",
        "\n",
        "plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
        "\n",
        "plt.title('ROC Curve Analysis for class 1', fontweight='bold', fontsize=15)\n",
        "plt.legend(prop={'size':10}, loc='lower right')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reference of TFIDF Vectorizer code"
      ],
      "metadata": {
        "id": "LMqQ2y80L8TX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfN59FCxS08t"
      },
      "source": [
        "# # TfidfVectorizer \n",
        "# # CountVectorizer\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
        "# import pandas as pd\n",
        "# def tfidf_columns(dataf) :\n",
        " \n",
        "#   # countvectorizer = CountVectorizer(analyzer= 'word', stop_words='english',vocabulary=vocab)\n",
        "#   tfidfvectorizer = TfidfVectorizer(analyzer='word',stop_words= 'english')\n",
        "#   # count_wm = countvectorizer.fit_transform(dataf['Sentence'])\n",
        "#   tfidf_wm = tfidfvectorizer.fit_transform(dataf['Sentence'])\n",
        "#   # print(count_wm)\n",
        "#   #retrieve the terms found in the corpora\n",
        "#   # if we take same parameters on both Classes(CountVectorizer and TfidfVectorizer) , it will give same output of get_feature_names() methods)\n",
        "#   #count_tokens = tfidfvectorizer.get_feature_names() # no difference\n",
        "#   # count_tokens = countvectorizer.get_feature_names()\n",
        "#   # print(count_tokens)\n",
        "#   tfidf_tokens = tfidfvectorizer.get_feature_names()\n",
        "#   # df_countvect = pd.DataFrame(data = count_wm.toarray(),columns = count_tokens)\n",
        "#   df_tfidfvect = pd.DataFrame(data = tfidf_wm.toarray(),columns = tfidf_tokens)\n",
        "#   # print(\"Count Aspects\\n\")\n",
        "#   # print(df_countvect)\n",
        "#   print(\"\\nTD-IDF Sentences\\n\")\n",
        "#   print(df_tfidfvect)\n",
        "#   return df_tfidfvect\n",
        "\n",
        "# t1=tfidf_columns(train_data)\n",
        "# t2=tfidf_columns(test_data)\n",
        "\n",
        "\n",
        "# # df_countvect['lee'].head(10)\n",
        "# print(type(t1))\n",
        "# vocab_train=t1.columns\n",
        "# new_vocab_train=[x for x in vocab_train if not any(c.isdigit() for c in x)]\n",
        "# type(vocab_train)\n",
        "# new_vocab_train.pop(0)\n",
        "# vocab_test=t2.columns\n",
        "# new_vocab_test=[x for x in vocab_test if not any(c.isdigit() for c in x)]\n",
        "# new_vocab_test.pop(0)\n",
        "# def tfidf_with_vocab(dataf,vocab) :\n",
        " \n",
        "#   # countvectorizer = CountVectorizer(analyzer= 'word', stop_words='english',vocabulary=vocab)\n",
        "#   tfidfvectorizer = TfidfVectorizer(analyzer='word',stop_words= 'english',vocabulary=vocab)\n",
        "#   # count_wm = countvectorizer.fit_transform(dataf['Sentence'])\n",
        "#   tfidf_wm = tfidfvectorizer.fit_transform(dataf['Sentence'])\n",
        "#   # print(count_wm)\n",
        "#   #retrieve the terms found in the corpora\n",
        "#   # if we take same parameters on both Classes(CountVectorizer and TfidfVectorizer) , it will give same output of get_feature_names() methods)\n",
        "#   #count_tokens = tfidfvectorizer.get_feature_names() # no difference\n",
        "#   # count_tokens = countvectorizer.get_feature_names()\n",
        "#   # print(count_tokens)\n",
        "#   tfidf_tokens = tfidfvectorizer.get_feature_names()\n",
        "#   # df_countvect = pd.DataFrame(data = count_wm.toarray(),columns = count_tokens)\n",
        "#   df_tfidfvect = pd.DataFrame(data = tfidf_wm.toarray(),columns = tfidf_tokens)\n",
        "#   # print(\"Count Aspects\\n\")\n",
        "#   # print(df_countvect)\n",
        "#   print(\"\\nTD-IDF Sentences\\n\")\n",
        "#   print(df_tfidfvect)\n",
        "#   return df_tfidfvect\n",
        "# # print(train_vocab)\n",
        "# vocab_for_tfidf = np.hstack((new_vocab_train,new_vocab_test))\n",
        "# len(vocab_for_tfidf)\n",
        "# type(vocab_for_tfidf)\n",
        "# vocab_for_tfidf=np.unique(vocab_for_tfidf)\n",
        "# len(vocab_for_tfidf)\n",
        "# t1=tfidf_with_vocab(train_data,vocab_for_tfidf)\n",
        "# t2=tfidf_with_vocab(test_data,vocab_for_tfidf)\n",
        "\n",
        "# # t1=tfidf_with_vocab(train_data,new_vocab_train)\n",
        "# # t2=tfidf_with_vocab(test_data,new_vocab_test)\n",
        "# Sentence = 'hi i am gentleman'\n",
        "# party = 'gentleman'\n",
        "# search(Sentence,party)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}